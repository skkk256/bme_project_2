{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set for jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dataset import get_loader\n",
    "from modules.utils import imsshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image count in train path :1394\n",
      "image count in val path :200\n",
      "image count in test path :204\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_loader(image_root_path='./ACDC-2D-All/train/', batch_size=32, mode='train')\n",
    "val_loader = get_loader(image_root_path='./ACDC-2D-All/val/', batch_size=32, mode='val')\n",
    "test_loader = get_loader(image_root_path='./ACDC-2D-All/test/', batch_size=32, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f3f19804850>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n",
      "the size of image and gt after transform are torch.Size([1, 256, 256]) and torch.Size([1, 256, 256])\n",
      "the size of image and gt before transform are (256, 256) and (256, 256)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "uniform_ expects to return a [from, to) range, but found from=2 > to=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(itertools\u001b[39m.\u001b[39mislice(loader, idx, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m batch\n\u001b[0;32m----> 7\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(train_loader, \u001b[39m0\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m))\n\u001b[1;32m      8\u001b[0m image, seg_gt \u001b[39m=\u001b[39m batch\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(image\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/bme/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/bme/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/bme/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/bme/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/shengkuan/project2/modules/dataset.py:99\u001b[0m, in \u001b[0;36mImageFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m Transform\u001b[39m.\u001b[39mappend(T\u001b[39m.\u001b[39mCenterCrop(\n\u001b[1;32m     96\u001b[0m     (\u001b[39mint\u001b[39m(CropRange \u001b[39m*\u001b[39m aspect_ratio), CropRange)))\n\u001b[1;32m     97\u001b[0m Transform \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mCompose(Transform)\n\u001b[0;32m---> 99\u001b[0m image \u001b[39m=\u001b[39m Transform(image)\n\u001b[1;32m    101\u001b[0m \u001b[39m# Be careful: when you do geometric transformation on the original image,you need to do\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39m# the same transform on the gt, to keep the consistency.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m seg_gt \u001b[39m=\u001b[39m Transform(seg_gt)\n",
      "File \u001b[0;32m~/anaconda3/envs/bme/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/bme/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/bme/lib/python3.8/site-packages/torchvision/transforms/transforms.py:1351\u001b[0m, in \u001b[0;36mRandomRotation.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m         fill \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fill]\n\u001b[0;32m-> 1351\u001b[0m angle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegrees)\n\u001b[1;32m   1353\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mrotate(img, angle, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterpolation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpand, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcenter, fill)\n",
      "File \u001b[0;32m~/anaconda3/envs/bme/lib/python3.8/site-packages/torchvision/transforms/transforms.py:1333\u001b[0m, in \u001b[0;36mRandomRotation.get_params\u001b[0;34m(degrees)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m   1327\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_params\u001b[39m(degrees: List[\u001b[39mfloat\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m   1328\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get parameters for ``rotate`` for a random rotation.\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m \n\u001b[1;32m   1330\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m \u001b[39m        float: angle parameter to be passed to ``rotate`` for random rotation.\u001b[39;00m\n\u001b[1;32m   1332\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1333\u001b[0m     angle \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(torch\u001b[39m.\u001b[39;49mempty(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49muniform_(\u001b[39mfloat\u001b[39;49m(degrees[\u001b[39m0\u001b[39;49m]), \u001b[39mfloat\u001b[39;49m(degrees[\u001b[39m1\u001b[39;49m]))\u001b[39m.\u001b[39mitem())\n\u001b[1;32m   1334\u001b[0m     \u001b[39mreturn\u001b[39;00m angle\n",
      "\u001b[0;31mRuntimeError\u001b[0m: uniform_ expects to return a [from, to) range, but found from=2 > to=0"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def fetch_batch_sample(loader, idx):\n",
    "    batch = next(itertools.islice(loader, idx, None))\n",
    "    return batch\n",
    "\n",
    "batch = next(itertools.islice(train_loader, 0, None))\n",
    "image, seg_gt = batch\n",
    "print(image.shape)\n",
    "print(seg_gt.shape)\n",
    "imsshow(image[:,0,  :, :])\n",
    "imsshow(seg_gt[:,0, :, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
